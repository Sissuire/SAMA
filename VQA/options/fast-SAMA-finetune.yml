
name: SAMA-baseline-finetune-youtube
split_seed: 10

num_epochs: 30
l_num_epochs: 0
warmup_epochs: 2.5
ema: true
save_model: true
train_batch_size: 12
train_num_workers: 6
test_batch_size: 1
test_num_workers: 6

stype: sama  # [sama, sama-c, sama-mix, sama-swm, sama-spm, fragments==others]

data:
    # database: livevqc
    # type: FineTuneDataset
    # anno_file: ./examplar_data_labels/LIVE_VQC/mylabels.txt
    # data_prefix: PATH_TO_DATA/LIVE-VQC
    # train:
    #     sample_types:
    #         fragments:
    #             fragments_h: 7
    #             fragments_w: 7
    #             fsize_h: 32
    #             fsize_w: 32
    #             aligned: 32
    #             clip_len: 32
    #             frame_interval: 2
    #             num_clips: 1
    # test:
    #     sample_types:
    #         fragments:
    #             fragments_h: 7
    #             fragments_w: 7
    #             fsize_h: 32
    #             fsize_w: 32
    #             aligned: 32
    #             clip_len: 32
    #             frame_interval: 2
    #             num_clips: 4


    # database: kv1k
    # type: FineTuneDataset
    # anno_file: ./examplar_data_labels/KoNViD/mylabels.txt
    # data_prefix: PATH_TO_DATA/KoNViD
    # train:
    #     sample_types:
    #         fragments:
    #             fragments_h: 7
    #             fragments_w: 7
    #             fsize_h: 32
    #             fsize_w: 32
    #             aligned: 32
    #             clip_len: 32
    #             frame_interval: 2
    #             num_clips: 1
    # test:
    #     sample_types:
    #         fragments:
    #             fragments_h: 7
    #             fragments_w: 7
    #             fsize_h: 32
    #             fsize_w: 32
    #             aligned: 32
    #             clip_len: 32
    #             frame_interval: 2
    #             num_clips: 4

    database: youtube
    type: FineTuneDataset
    anno_file: ./examplar_data_labels/YouTubeUGC/mylabels.txt
    data_prefix: PATH_TO_DATA/YouTube
    train:
        sample_types:
            fragments:
                fragments_h: 7
                fragments_w: 7
                fsize_h: 32
                fsize_w: 32
                aligned: 32
                clip_len: 32
                frame_interval: 2
                num_clips: 1
    test:
        sample_types:
            fragments:
                fragments_h: 7
                fragments_w: 7
                fsize_h: 32
                fsize_w: 32
                aligned: 32
                clip_len: 32
                frame_interval: 2
                num_clips: 4

model:
    type: DiViDeAddEvaluator
    args:
        backbone:
            fragments:
                checkpoint: false
                pretrained: 
        backbone_size: swin_tiny_grpb
        backbone_preserve_keys: fragments
        divide_head: false
        vqa_head:
            in_channels: 768
            hidden_channels: 64
            
optimizer:
    lr: !!float 1e-3
    backbone_lr_mult: !!float 1e-1
    wd: 0.05
        
load_path: PATH_TO_MODEL/pretrained_weights/SAMA-baseline_val-ltest_s_dev_v0.0.pth

